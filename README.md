# AI Security Resources

This repository is a **curated list of resources on AI security**. It is designed to serve as a resource hub for researchers, practitioners, and anyone interested in understanding and improving the security of AI systems.

With the increasing use of AI in critical domains, the risks associated with adversarial attacks, data poisoning, and system misuse have become more pronounced. This repository consolidates resources to build awareness and improve understanding of AI security.

> **Note**: Contributions to this repository are not accepted. Please feel free to fork it if you'd like to create your own version.

## AI Red Teaming - Adversarial testing tools

* [Python Risk Indentification for Generative AI - PyRIT!](https://azure.github.io/PyRIT/) PyRIT is an open-source framework developed by [Microsoft's AI Red Team](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems/) to be a flexible and extensible tool that can be used to orchestrate an observe the security and safety vulnerabilities of generative AI systems in a variety of ways.
[PyRIT GitHub repo](https://github.com/Azure/PyRIT)

* [AttackText](https://textattack.readthedocs.io/en/master/) is a Python framework for adversarial attacks, adversarial training, and data augmentation in Natural Language Proccessing. TextAttack makes experimenting with the robustness of Natural Language Proccessing models seamless, fast, and easy. It’s also useful for Natural Language Proccessing model training, adversarial training, adversarial testing with prompt engineering, and data augmentation.

## Frameworks

* Databricks AI Security Framework [DASF](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf) | The Databricks AI Security Framework is designed to help you create an end-to-end risk profile for your organization’s AI deployment needs.

* HITRUST | [AI Security Assessment](https://hitrustalliance.net/assessments-and-certifications/aisecurityassessment) | The AI Security Assessment is designed to provide AI platform and service providers with relevant, prescriptive, and practical security controls and methodology to confidently adopt and secure AI technologies.

* [OWASP Top 10 for LLM Applications 2025](https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/) | The OWASP Top 10 for Large Language Model Applications started in 2023 as a community-driven effort to highlight and address security issues specific to AI applications. 

* [MITRE ATLAS](https://atlas.mitre.org/) | ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) is a globally accessible, living knowledge base of adversary tactics and techniques against Al-enabled systems based on real-world attack observations and realistic demonstrations from Al red teams and security groups.

## Standards and Publications

* [ISO/IEC 42001:2023 Artificial Intelligence Management System](https://www.iso.org/standard/81230.html) | ISO/IEC 42001 is an international standard that specifies requirements for establishing, implementing, maintaining, and continually improving an Artificial Intelligence Management System (AIMS) within organizations.

* [ISO/IEC 22989:2022 Artificial intelligence concepts and terminology](https://www.iso.org/standard/74296.html) | ISO/IEC 22989 is a publication that establishes terminology for AI and describes concepts in the field of AI. This document can be used in the development of other standards and in support of communications among diverse, interested parties or stakeholders.

## Reglulation

* [Australian Voluntary AI Safety Standard](https://www.industry.gov.au/publications/voluntary-ai-safety-standard) | The Australian Voluntary AI Safety Standard helps Australian organisations develop and deploy AI systems safely and reliably | This Standard includen 10 voluntary AI guardrails and how to use them | [10 Voluntary Guardrails](https://www.industry.gov.au/publications/voluntary-ai-safety-standard/10-guardrails)

* [EU AI Act](https://artificialintelligenceact.eu/) | The EU Artificial Intelligence Act

## AI Ethical Principles

* [Australia’s AI Ethics Principles](https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-principles/australias-ai-ethics-principles) | Australia’s 8 Artificial Intelligence (AI) Ethics Principles are designed to ensure AI is safe, secure and reliable.

## Trustworthy AI Principles

* [OECD AI Principles overview](https://oecd.ai/en/ai-principles) | The OECD AI Principles promote use of AI that is innovative and trustworthy and that respects human rights and democratic values. Adopted in May 2019, they set standards for AI that are practical and flexible enough to stand the test of time.

## Books

* [Not with a Bug, But with A Sticker](https://www.wiley.com/en-us/Not+with+a+Bug%2C+But+with+a+Sticker%3A+Attacks+on+Machine+Learning+Systems+and+What+To+Do+About+Them-p-9781119883999)

* [Adversarial AI Attacks, Mitigations, and Defense Strategies](https://www.packtpub.com/en-au/product/adversarial-ai-attacks-mitigations-and-defense-strategies-9781835088678?srsltid=AfmBOoqCrkIs2JWvcMSjdAV3nSLcxlUgr9iUnZrjRl7tjxiOZ3y6FJwU)

* [Co-Intelligence: Living and Working with AI](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/)

* [AI Act Compact](https://books.google.com.au/books/about/AI_Act_compact.html?id=EK04EQAAQBAJ&redir_esc=y)


